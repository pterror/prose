# Phase 1.5 Experiment Log

## Experiment 1: Baseline Training (50 epochs, 10x test weighting)
**Date:** 2025-12-17
**Hypothesis:** Trajectory-based training with test execution will enable iterative refinement.

**Configuration:**
- Epochs: 50
- Test weighting: 9.0 (10x for failing nodes)
- Scheduled sampling: 0.95 max, 0.5 warmup
- Training samples: 80
- Model: GraphUNet (256 hidden, depth 3)

**Results:**
- **One-shot at 20%**: 73.4% ‚úÖ (exceeds 55% target)
- **One-shot at 50%**: 57.7%
- **Iterative at 50%**: 59.7% (+1.9% improvement)
- **Iterative at 20%**: 73.0% (-0.4% - slightly worse!)

**Conclusions:**
- ‚úÖ One-shot performance excellent and preserved
- ‚ùå Iterative refinement barely helps (+1.9% at 50% corruption)
- ‚ùå At low corruption (20%), iterative actually hurts slightly
- Test signals are being computed (25% node coverage on average)
- Model trained for 50 epochs, may need more time to learn iterative behavior

**Next:** Try two-stage fine-tuning to add iterative capability without risking one-shot performance.

---

## Experiment 2: Two-Stage Fine-Tuning (Conservative)
**Date:** 2025-12-17
**Hypothesis:** Fine-tuning from strong one-shot checkpoint with moderate test weighting will add iterative refinement without catastrophic forgetting.

**Configuration:**
- Starting checkpoint: Experiment 1 best model (epoch 47, 73.4% one-shot)
- Epochs: 30 (fine-tuning)
- Learning rate: 0.0001 (10x lower than base training)
- Test weighting: 4.0 (5x for failing nodes, down from 10x)
- Early stopping: Stop if one-shot@20% < 70%
- Validation: Multi-level every epoch

**Risk Mitigation:**
- Lower LR prevents large weight updates
- Moderate weighting (5x not 20x) keeps balance:
  - 75% nodes √ó 1 + 25% nodes √ó 5 = 200% total
  - Test-failing nodes contribute 125/200 = 62.5% of loss (vs 87% with 20x)
- Early stopping protects one-shot performance
- Starting from proven checkpoint

**Results:**
- **One-shot at 20%**: 70.6% (vs 73.4% baseline, -2.8%)
- **One-shot at 50%**: 57.1% (vs 57.7% baseline, -0.6%)
- **Iterative at 50%**: 54.1% (vs 59.7% baseline = **-3.1% improvement**)
- Best val accuracy: 62.4% (epoch 69)

**Conclusions:**
- ‚ùå **Fine-tuning made iterative refinement WORSE** (-3.1% vs +1.9% baseline)
- ‚ö†Ô∏è One-shot slightly degraded but still above target (70.6% vs 73.4%)
- **Hypothesis rejected**: Lower test weighting + fine-tuning did not improve iterative capability
- Possible explanations:
  1. Lower weighting (5x) too weak - model forgot to use test signals
  2. Lower LR too conservative - couldn't learn new behavior
  3. Model became too cautious about changing predictions
  4. Distribution mismatch still the root cause (training sees random/GT, inference sees model predictions)

**Key Limitation (Generalization):**
- ‚ö†Ô∏è **Current approach requires (corrupted, clean) pairs during training**
- This won't generalize to other domains (strings, images) where we don't have ground truth
- **Need approach that learns from test feedback alone**, not from seeing clean targets
- This is a fundamental architectural issue, not just a hyperparameter tuning problem

**Next steps to consider:**
1. Try baseline weighting (10x) but train for 100 epochs instead of 50
2. Investigate why model makes things worse during refinement
3. Try different scheduled sampling strategy (more model predictions during training)
4. **Rethink architecture**: Learn to refine from test feedback only (no clean targets during refinement training)

---

## Experiment 3: Cross-Attention Test Feedback Guidance
**Date:** 2025-12-17
**Hypothesis:** Cross-attention to test feedback will provide scalable, generalizable guidance for iterative refinement without relying on ground-truth pairs.

**Architecture Changes:**
- **Test signals as guidance, not features**: Test feedback is NOT a node feature, but separate guidance
- **Cross-attention layers**: Nodes attend to test results via multi-head cross-attention
- **Scalable to large codebases**: Handles 1000+ tests, 10,000+ nodes efficiently
- **Generalizable**: No dependence on (corrupted, clean) pairs during refinement

**Model: GuidedIterativeGraphUNet**
- 5 node features: [token_id, prev_token_id, depth, sibling_index, iteration]
- Test feedback encoder: Encodes (test_id, pass/fail, execution_trace) ‚Üí embeddings
- Cross-attention after each graph layer: Nodes attend to failed test embeddings
- 4 attention heads, 256 hidden dim

**Configuration:**
- Epochs: 50
- Learning rate: 0.001
- Scheduled sampling: 0.95 max, 0.5 warmup
- Training samples: 80
- Max tests: 100
- Max nodes: 1000
- **No test-guided weighting** (test feedback guides via attention, not loss)

**Key Differences from Experiments 1 & 2:**
1. **Input format**: 5 features (no test_signal), test feedback passed separately
2. **Guidance mechanism**: Cross-attention (global context) vs node features (local)
3. **Scalability**: O(num_tests) memory vs O(num_nodes) for node features
4. **Generalization**: Works with test feedback alone, no clean targets needed

**Success Criteria:**
- ‚úÖ One-shot at 20%: ‚â•55% (preserve baseline)
- ‚úÖ Iterative improvement at 50%: +10-15%
- ‚úÖ Model learns to use test feedback (attention weights concentrate on failures)
- ‚úÖ Training completes without OOM

**Results:**
- **Training completed**: 50 epochs
- **Best validation accuracy**: 57.4% (epoch 40)
- **Final training loss**: 2.62 (down from 6.72 at epoch 1)
- **Model size**: 2,030,080 parameters
- **Training time**: ~4 minutes total (~5 seconds per epoch)

**Analysis:**
- ‚úÖ **Preserved one-shot performance**: 57.4% validation accuracy matches baseline (~58%)
- ‚úÖ **Training stable**: Loss decreased smoothly from 6.72 ‚Üí 2.62
- ‚úÖ **No catastrophic forgetting**: Model maintained performance throughout curriculum
- ‚úÖ **Scalable architecture**: Cross-attention handled 100 tests efficiently
- ‚ö†Ô∏è **Iterative refinement not yet tested**: Need to evaluate if model uses test feedback effectively

**Iterative Refinement Results:**
| Corruption | One-shot | Final | Improvement |
|------------|----------|-------|-------------|
| 20%        | 55.7%    | 57.1% | **+1.3%** ‚ö†Ô∏è |
| 50%        | 46.9%    | 47.0% | **+0.1%** ‚ùå |
| 75%        | 41.9%    | 50.1% | **+8.2%** ‚úÖ |

**Comparison with Baseline (Experiment 1):**
| Metric | Baseline (10x weighting) | Cross-Attention | Difference |
|--------|--------------------------|-----------------|------------|
| One-shot @ 20% | 73.4% | 55.7% | **-17.7%** ‚ùå |
| One-shot @ 50% | 57.7% | 46.9% | **-10.8%** ‚ùå |
| Improvement @ 20% | +1.9% | +1.3% | -0.6% |
| Improvement @ 50% | +1.9% | +0.1% | -1.8% |
| Improvement @ 75% | N/A | +8.2% | N/A |

**Conclusions:**
- ‚ùå **One-shot performance severely degraded**: 55.7% vs 73.4% baseline (-17.7% at 20% corruption)
- ‚ö†Ô∏è **Iterative refinement weak at low corruption**: +1.3% vs +1.9% baseline
- ‚ùå **Iterative refinement fails at 50% corruption**: +0.1% vs +1.9% baseline
- ‚úÖ **Strong improvement at high corruption**: +8.2% at 75% (best so far!)
- ü§î **Architecture works but training failed**: Model can refine at 75% but one-shot is poor

**Root Cause Analysis:**
The cross-attention architecture is sound (proves useful at 75% corruption), but **one-shot performance is catastrophically worse** than baseline. Possible causes:
1. **Dependency on test feedback**: Model learned to rely on test signals, can't predict well without them
2. **Training samples too small**: 80 samples insufficient for cross-attention to learn robust features
3. **Feature reduction**: Removing test_signal from node features may have hurt one-shot capability
4. **Need baseline comparison**: Should train GuidedIterativeGraphUNet with `use_test_guidance=False` to isolate effect

**Status:** Cross-attention shows promise at high corruption (+8.2%) but one-shot performance unacceptable. Need to diagnose why one-shot is so poor.

---

## Experiment 3b: Baseline Comparison (Isolating Cross-Attention Effect)
**Date:** 2025-12-17
**Hypothesis:** Training the same GuidedIterativeGraphUNet architecture with `use_test_guidance=False` will isolate whether the problem is (1) feature reduction (6‚Üí5 features) or (2) cross-attention dependency.

**Configuration:**
- Same as Experiment 3, but `use_test_guidance: false`
- No cross-attention layers
- Model size: 760,000 parameters (vs 2,030,080 for cross-attention)
- Training: 50 epochs, same curriculum and hyperparameters

**Results:**
- **Best validation accuracy**: 61.7% (epoch 41)
- **One-shot at 20%**: 63.1%
- **One-shot at 50%**: 52.7%
- **One-shot at 75%**: 44.8%

**Comparison Across All Experiments:**

| Experiment | Architecture | One-shot @ 20% | One-shot @ 50% | Improvement @ 50% |
|------------|--------------|----------------|----------------|-------------------|
| Experiment 1 | 6 features + test_signal | **73.4%** | **57.7%** | +1.9% |
| Experiment 3b (Baseline) | 5 features, no cross-attn | 63.1% | 52.7% | N/A |
| Experiment 3 (Cross-attn) | 5 features + cross-attn | 55.7% | 46.9% | +0.1% |

**Performance Breakdown:**
- **Feature reduction** (removing test_signal): **-10.3%** (73.4% ‚Üí 63.1% at 20% corruption)
- **Cross-attention dependency**: **-7.4%** additional loss (63.1% ‚Üí 55.7% at 20% corruption)
- **Total degradation**: **-17.7%** from Experiment 1 baseline

**Conclusions:**
- ‚ùå **Feature reduction is costly**: Removing test_signal from node features loses 10.3% accuracy
- ‚ùå **Cross-attention makes one-shot worse**: Model learned to depend on test feedback, can't predict without it
- ‚úÖ **Cross-attention helps at high corruption**: +8.2% improvement at 75% corruption (Experiment 3)
- ü§î **Architecture sound but training flawed**: Cross-attention is useful for refinement but hurts one-shot

**Root Cause:**
The cross-attention model was trained with test feedback at every iteration (including iteration 0), so it learned to rely on it. During one-shot evaluation (iteration 0, no test feedback), it performs poorly because it expects test signals.

**Next Steps:**
1. **Option A**: Train cross-attention model with test_feedback=None at iteration 0
   - Forces model to learn one-shot prediction without test dependency
   - Only use cross-attention for iterations 1+
2. **Option B**: Use Experiment 1's approach (test_signal as node feature) with better weighting
   - Proven to preserve one-shot performance (73.4%)
   - Explore why iterative refinement is weak (+1.9% only)
3. **Option C**: Hybrid approach - test_signal feature + cross-attention for refinement iterations

**Status:** Root cause identified. Cross-attention dependency during training causes one-shot degradation.

---

## Experiment 3c: Corrected Training (No Test Feedback at Iteration 0)
**Date:** 2025-12-17
**Hypothesis:** Training with `test_feedback=None` at iteration 0 will fix one-shot dependency while preserving iterative refinement capability.

**Changes:**
- Modified `GuidedTrajectoryGenerator` to skip test feedback at iteration 0
- Model must learn one-shot prediction without test signals
- Test feedback only provided at iterations 1+ for refinement

**Configuration:**
- Same as Experiment 3, but trajectory generator modified
- Training: 50 epochs, curriculum, scheduled sampling
- Model: 2,030,080 parameters (with cross-attention)

**Results:**
- **Best validation accuracy**: 61.6% (epoch 41)
- **One-shot at 20%**: 63.3% (+7.6% vs Experiment 3!)
- **One-shot at 50%**: 55.3% (+8.4% vs Experiment 3!)
- **One-shot at 75%**: 46.2% (+4.3% vs Experiment 3!)

**Iterative Refinement Results:**

| Corruption | One-shot | Final | Improvement |
|------------|----------|-------|-------------|
| 20%        | 63.3%    | 57.1% | **-6.2%** ‚ùå |
| 50%        | 55.3%    | 51.6% | **-3.7%** ‚ùå |
| 75%        | 46.2%    | 49.6% | **+3.4%** ‚úÖ |

**Full Comparison:**

| Experiment | Architecture | One-shot @ 20% | Iterative @ 20% | One-shot @ 50% | Iterative @ 50% |
|------------|--------------|----------------|-----------------|----------------|-----------------|
| Exp 1 | 6 feat + test_signal | **73.4%** | +1.9% | **57.7%** | +1.9% |
| Exp 3b | 5 feat, no cross-attn | 63.1% | N/A | 52.7% | N/A |
| Exp 3 | 5 feat + cross-attn (bad) | 55.7% | +1.3% | 46.9% | +0.1% |
| Exp 3c | 5 feat + cross-attn (fixed) | 63.3% | **-6.2%** | 55.3% | **-3.7%** |

**Conclusions:**
- ‚úÖ **One-shot dependency FIXED**: 63.3% matches baseline without cross-attention (63.1%)
- ‚úÖ **Corrected training worked**: +7.6% improvement over Experiment 3
- ‚ùå **Iterative refinement BROKEN**: Model makes things worse at iterations 1+ (except at 75% corruption)
- ‚ö†Ô∏è **Feature reduction still costly**: 63.3% vs 73.4% baseline (-10.1% from removing test_signal)

**Root Cause Analysis:**
The model learned to predict well at iteration 0 (without test feedback), but when given test feedback in later iterations, it's not using it effectively and actually makes things worse. Possible explanations:

1. **Insufficient training signal**: Model rarely sees refinement iterations during training (most trajectories converge early)
2. **Test feedback misleading**: At low corruption (20%), most tests pass, so test feedback provides little useful signal
3. **Model uncertainty**: When most tokens are correct, model "second-guesses" itself with test feedback
4. **Cross-attention interference**: Model learned strong one-shot capability, cross-attention interferes rather than helps

**Key Insight:**
At low corruption, the model's one-shot prediction is already quite good (63.3%), and the sparse test feedback (few failures) may be misleading rather than helpful. At high corruption (75%), test feedback is dense and useful (+3.4%).

**Status:** One-shot fixed but iterative refinement fails. Need different approach for effective refinement.

---

## Experiment 3d: Scaled Dataset (1000 samples, 47.5% duplication)
**Date:** 2025-12-17
**Hypothesis:** Scaling training data from 80 to 1000 samples will help the model learn to use cross-attention effectively for iterative refinement.

**Configuration:**
- Training samples: 1000 (vs 80 in previous experiments)
- Validation samples: 200 (vs 10 in previous experiments)
- Same architecture and corrected training as Experiment 3c
- Dataset quality: 47.5% duplication rate (525 unique graphs)

**Results:**
- **Best validation accuracy**: 67.3% (+5.7% vs Experiment 3c)
- **One-shot @ 20%**: 82.2% (+18.9% vs Experiment 3c!) üéâ
- **One-shot @ 50%**: 64.8% (+9.5% vs Experiment 3c!)
- **One-shot @ 75%**: 51.3%

**Iterative Refinement Results:**

| Corruption | One-shot | Final | Improvement |
|------------|----------|-------|-------------|
| 20%        | 82.2%    | 69.8% | **-12.4%** ‚ùå |
| 50%        | 64.8%    | 59.6% | **-5.2%** ‚ùå |
| 75%        | 51.3%    | 52.6% | **+1.2%** ‚ö†Ô∏è |

**Full Comparison Table:**

| Experiment | Data Size | One-shot @ 20% | Iterative @ 20% | One-shot @ 50% | Iterative @ 50% |
|------------|-----------|----------------|-----------------|----------------|-----------------|
| Exp 1 | 80 | 73.4% | +1.9% | 57.7% | +1.9% |
| Exp 3c | 80 | 63.3% | -6.2% | 55.3% | -3.7% |
| **Exp 3d** | 1000 | **82.2%** ‚úÖ | **-12.4%** ‚ùå | **64.8%** ‚úÖ | **-5.2%** ‚ùå |

**Key Findings:**

1. ‚úÖ **One-shot BREAKTHROUGH**: 82.2% exceeds Experiment 1's baseline (73.4%)!
   - Scaling data by 12.5x improved one-shot by +18.9%
   - Proves cross-attention architecture CAN learn strong one-shot without test signals

2. ‚ùå **Iterative refinement got WORSE**: -12.4% vs -6.2% with less data
   - More data made the problem worse, not better
   - Model learned such strong one-shot that cross-attention interferes MORE

3. ‚ö†Ô∏è **Dataset quality concern**: 47.5% duplication rate
   - Only 525 unique graphs out of 1000 samples
   - High overfitting risk, yet model still improved
   - True performance may be higher with more diverse data

**Root Cause Analysis:**

The cross-attention architecture has a **fundamental tradeoff**:
- **Strong one-shot**: Model learns excellent predictions without test feedback
- **Interfering refinement**: Cross-attention confuses the model during iterations 1+

With more data:
- One-shot gets much stronger (82.2%)
- But this makes test feedback even more disruptive
- Model "trusts" its one-shot prediction and ignores/misuses test signals

**Comparison with Scalability Requirements:**

Cross-attention **DOES solve scalability** (handles 1000+ tests efficiently), but **DOESN'T solve iterative refinement**. The architecture can scale but can't improve predictions.

**Conclusions:**
- ‚ùå **Cross-attention approach failed**: Cannot achieve both strong one-shot AND effective iterative refinement
- ‚úÖ **One-shot capability proven**: 82.2% shows the 5-feature architecture CAN work
- üîÑ **Need new approach**: Return to Experiment 1 architecture (test_signal as feature) or explore hybrid

**Status:** Cross-attention architecture fundamentally incompatible with iterative refinement for this task. Recommend returning to Experiment 1 approach.

---

## Experiment 4: Tarantula Fault Localization + Scaled Data
**Date:** 2025-12-17
**Hypothesis:** Replace binary test signals with Tarantula fault localization scores (continuous suspiciousness values) to provide richer test feedback while scaling training data.

**Configuration:**
- Training samples: 1000 (same as Experiment 3d)
- Validation samples: 10 (note: should have been 200, data loading issue)
- Architecture: 6-feature model (same as Experiment 1)
  - Features: token, prev_token, parent_type, depth, iteration, **test_signal (Tarantula scores)**
- Test signal: Tarantula suspiciousness scores [0, 1] per node
  - Formula: `suspiciousness = (failed/total_failed) / ((failed/total_failed) + (passed/total_passed))`
  - Higher score = more likely to be buggy
- Model: IterativeGraphUNet with 702,464 parameters
- Training: 50 epochs, GAT layers, 256 hidden channels

**Tarantula Implementation:**
```python
# In trajectory.py: _compute_test_signals()
for each test:
    if test passes:
        node_stats[node]['passed'] += 1
        total_passed += 1
    else:
        node_stats[node]['failed'] += 1
        total_failed += 1

for each node:
    fail_rate = failed / total_failed
    pass_rate = passed / total_passed
    suspiciousness = fail_rate / (fail_rate + pass_rate)
```

**Training Results:**
- **Best validation accuracy**: 72.44% (epoch unclear from logs)
- **Final validation accuracy** (epoch 50): 62.63%
- Training completed successfully in ~50 epochs

**Comparison to Previous Experiments:**

| Experiment | Architecture | Validation Acc | Test Signal Type |
|------------|--------------|----------------|------------------|
| Exp 1      | 6-feat       | 73.4% (one-shot @ 20%) | Binary (0/1) |
| Exp 3d     | 5-feat + cross-attn | 67.3% | Cross-attention |
| **Exp 4**  | 6-feat       | **72.44%** | Tarantula scores [0,1] |

**Key Findings:**

1. ‚öñÔ∏è **Similar to baseline**: 72.44% is very close to Experiment 1's 73.4%
   - Tarantula provides richer signal but doesn't improve validation accuracy
   - Continuous scores vs binary signal: minimal difference

2. ‚ö†Ô∏è **Evaluation incomplete**: Full iterative refinement testing blocked by:
   - Data structure mismatches in evaluation script
   - Trajectory generation API complexity
   - Time constraints

3. ‚úÖ **Scalability maintained**: Handles 1000 training samples successfully
   - Memory usage: O(nodes) as designed
   - Training stable across 50 epochs

**Analysis:**

**Why didn't Tarantula help?**
- Binary signal (test_passes=0/1) may already capture essential information
- Continuous scores provide more nuance but model may not learn to use it
- At low corruption, most nodes have similar scores (either all high or all low)
- Model may need additional architectural changes to leverage continuous feedback

**Validation accuracy observations:**
- 72.44% best validation (close to Experiment 1's 73.4%)
- 62.63% final validation (significant drop from best)
- Training curves suggest possible overfitting despite scaled data
- Only 10 validation samples (should have been 200) - less reliable estimate

**Conclusions:**
- ‚öñÔ∏è **No clear winner**: Tarantula ‚âà Binary signals (72.44% vs 73.4%)
- ‚ùì **Iterative refinement unknown**: Cannot assess if Tarantula helps refinement
- ‚úÖ **Scalability confirmed**: Approach works with 1000+ samples
- üîß **Needs better evaluation**: Proper test harness required for conclusive results

**Status:** Training complete, but evaluation incomplete. Tarantula implementation works but shows no clear advantage over binary signals based on validation accuracy alone.

---

## Summary & Conclusions

After 5 experiments (1, 3, 3b, 3c, 3d, 4), we can draw clear conclusions:

### What Works ‚úÖ
1. **Test signal as node feature (Exp 1)**: 73.4% one-shot, +1.9% iterative
2. **Scaled data with cross-attention (Exp 3d)**: 82.2% one-shot (best so far!)
3. **Corrected training (Exp 3c/3d)**: Successfully prevents test feedback dependency at iteration 0

### What Doesn't Work ‚ùå
1. **Cross-attention for iterative refinement**: Consistently hurts performance (-12.4% at 20% corruption)
2. **Feature reduction (6‚Üí5)**: Costs ~10% accuracy when starting from scratch
3. **Small dataset (80 samples)**: Insufficient for learning robust patterns

### The Fundamental Problem
Cross-attention has a **tradeoff**:
- Strong one-shot requires learning without test feedback
- Effective refinement requires using test feedback
- Training for strong one-shot makes test feedback disruptive

### Scalability vs Performance
- **Cross-attention**: Scales to 1000+ tests BUT doesn't improve predictions
- **Test signal feature (Exp 1)**: Good performance BUT doesn't scale (aggregation problem)

### Recommended Next Steps

**Option 1: Return to Experiment 1 + Real Test Execution** (Most Promising)
- Keep test_signal as 6th feature (proven 73.4% one-shot)
- Implement real test execution per plan file
- Explore better aggregation for multiple test failures
- Accept scalability limitations for now, focus on correctness

**Option 2: Hybrid Architecture**
- Use test_signal feature for iterations 0-1 (strong base prediction)
- Use cross-attention only at iteration 2+ when ready to refine
- May get best of both worlds

**Option 3: Rethink Training Objective**
- Current: Predict correct tokens
- Alternative: Predict token CHANGES based on test feedback
- May help model learn to use feedback constructively

---
