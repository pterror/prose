"""Quick script to examine denoising examples."""

import sys
from pathlib import Path

import torch
from torch_geometric.data import Batch

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.dataset import DenoisingGraphDataset
from src.data.asg_builder import NodeType
from src.models.graph_unet import GraphUNet


def render_graph_as_code(graph):
    """Render a graph back to Mini-Lisp code."""
    # This is a simplified version - just show node types
    node_types = graph.x.argmax(dim=1).tolist()
    type_names = [NodeType(t).name for t in node_types]
    return " ".join(type_names)


def analyze_predictions(model, dataset, device, num_examples=5):
    """Show concrete examples of denoising."""
    model.eval()

    print("=" * 80)
    print("DENOISING EXAMPLES")
    print("=" * 80)

    for i in range(min(num_examples, len(dataset))):
        # Get a single sample
        corrupted, original = dataset[i]

        # Add batch dimension
        corrupted_batch = Batch.from_data_list([corrupted]).to(device)
        original_batch = Batch.from_data_list([original]).to(device)

        # Get prediction logits from model
        with torch.no_grad():
            logits = model.forward_full(corrupted_batch)  # Returns logits, not Data object

        # Get node type predictions
        corrupted_types = corrupted.x.cpu().numpy()  # Input is node type IDs
        pred_types = logits.argmax(dim=1).cpu().numpy()  # Convert logits to predictions
        original_types = original.x.cpu().numpy()  # Ground truth is node type IDs

        # Calculate accuracy for this sample
        correct = (pred_types == original_types).sum()
        total = len(original_types)
        accuracy = correct / total

        # Count errors by type
        errors = pred_types != original_types
        num_errors = errors.sum()

        print(f"\n{'=' * 80}")
        print(f"Example {i + 1}")
        print(f"{'=' * 80}")
        print(f"Graph size: {total} nodes")
        print(f"Node accuracy: {correct}/{total} = {accuracy:.2%}")
        print(f"Errors: {num_errors}")

        if num_errors > 0:
            print(f"\nError breakdown:")
            error_indices = errors.nonzero()[0]
            for idx in error_indices[:5]:  # Show first 5 errors
                # Handle MASK token (ID 8) specially
                def get_type_name(type_id):
                    return "MASK" if type_id == 8 else NodeType(type_id).name

                corr_type = get_type_name(corrupted_types[idx])
                pred_type = get_type_name(pred_types[idx])
                true_type = get_type_name(original_types[idx])

                was_masked = corrupted_types[idx] == 8  # MASK token

                print(f"  Node {idx}:")
                print(f"    Corrupted: {corr_type}{'  [MASKED]' if was_masked else ''}")
                print(f"    Predicted: {pred_type}")
                print(f"    True:      {true_type}")
                print(f"    Status:    {'WRONG' if pred_type != true_type else 'CORRECT'}")

        print()


def main():
    # Load checkpoint
    checkpoint_path = Path("checkpoints/best_model.pt")
    print(f"Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location="cpu")

    config = checkpoint["config"]

    # Create model
    device = torch.device("cpu")
    model = GraphUNet(
        in_channels=config["model"]["in_channels"],
        hidden_channels=config["model"]["hidden_channels"],
        out_channels=config["model"]["out_channels"],
        depth=config["model"]["depth"],
        pool_ratio=config["model"]["pool_ratio"],
        num_node_types=config["model"]["num_node_types"],
        layer_type=config["model"]["layer_type"],
    ).to(device)

    model.load_state_dict(checkpoint["model_state_dict"])

    # Load test dataset
    test_dataset = DenoisingGraphDataset(
        Path("data/processed/test"),
        corruption_rate=config["data"]["corruption_rate"],
        mask_token_id=config["data"]["mask_token_id"],
        seed=config["seed"] + 2,
    )

    print(f"Test dataset size: {len(test_dataset)}")
    print(f"Corruption rate: {config['data']['corruption_rate']}")

    # Analyze examples
    analyze_predictions(model, test_dataset, device, num_examples=10)

    # Overall statistics
    print("\n" + "=" * 80)
    print("OVERALL ANALYSIS")
    print("=" * 80)
    print(f"20% of nodes are corrupted (masked)")
    print(f"Model achieves ~86% node accuracy")
    print(f"")
    print(f"This means:")
    print(f"  - On correctly predicted nodes (~86%): model reconstructs successfully")
    print(f"  - On incorrectly predicted nodes (~14%): model makes wrong predictions")
    print(f"")
    print(f"14% error rate â‰ˆ 70% of the 20% corrupted nodes")
    print(f"So the model correctly predicts ~30% of masked nodes perfectly,")
    print(f"and makes errors on ~14% of all nodes (mostly on corrupted nodes).")


if __name__ == "__main__":
    main()
