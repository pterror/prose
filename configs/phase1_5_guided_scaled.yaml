model:
  hidden_channels: 256
  depth: 3
  pool_ratio: 0.5
  layer_type: GAT
  max_iterations: 5
  max_tests: 100
  max_nodes: 1000
  num_attention_heads: 4
  use_test_guidance: true  # Cross-attention enabled

training:
  epochs: 50
  batch_size: 1
  lr: 0.001
  weight_decay: 0.0001
  gradient_accumulation_steps: 8
  warmup_epochs: 5

# Scheduled sampling (same as previous)
scheduled_sampling_max: 0.95
scheduled_sampling_warmup: 0.5

data:
  train_dir: data/phase1_5/train  # 1000 samples
  val_dir: data/phase1_5/val      # 200 samples
  vocabulary: data/phase1_5/vocabulary.json

# KEY: test_feedback=None at iteration 0 (corrected training from Experiment 3c)
# This prevents one-shot dependency while allowing cross-attention for refinement

logging:
  log_dir: runs/phase1_5_guided_scaled
  checkpoint_dir: checkpoints/phase1_5_guided_scaled
  save_every: 5
  log_every: 100  # Log less frequently with more data

device: cuda
seed: 42
